# 深度学习

[TOC]

## 深度学习基础

### 基本组件

- 感知器：基本神经单元

- 激活函数：去线性化，构建更复杂系统

- 卷积层：图像特征提取


- 池化层：池化一般分为**最大池化层（max pooling）**，**平均池化层（average pooling）**，**特征融入**。


- 全连接层：模型推断


- 目标函数：衡量该预测值与真实样本标记标记之间的误差

  > 交叉熵损失函数：分类问题
  >
  > L2损失函数：回归问题

- Softmax回归：实现分类

### 激活函数

​	激活函数的作用是能够给神经网络加入一些**非线性**因素，使得神经网络可以更好地解决较为复杂的问题。

- **Sigmod （逻辑函数，用于二分类问题）**

  Sigmoid函数曾被广泛地应用。去线性化，Sigmoid函数的输出映射在**(0,1)**之间，单调连续，输出范围有限，优化稳定，可以用作输出层，求导容易，远离坐标远点后存在**梯度消失**问题。

![img](file:////tmp/wps-yeapht/ksohtml/wps6rY7KS.jpg)            ![img](file:////tmp/wps-yeapht/ksohtml/wpsTcZbSu.jpg)

- **Softmax（用于多分类问题）**

 

 

- **ReLU（线性整流函数）**

  在输入为正数的时候，不**存在梯度饱和问题；计算速度要快很多，**sigmod和tanh要计算指数，计算速度会比较慢，当输入是负数的时候，ReLU是**完全不被激活**的，反向传播过程中，输入负数，**梯度就会完全到0**，ReLU函数也不**是以0为中心的函数。**

  ![img](file:////tmp/wps-yeapht/ksohtml/wpszmihZ6.jpg)      ![img](file:////tmp/wps-yeapht/ksohtml/wpsZgxn6I.jpg)

- **Tanh（双曲正切函数）**

  tanh是双曲正切函数，tanh函数和sigmod函数的曲线是比较相近的。首先相同的是，这两个函数在输入很大或是很小的时候，**输出都几乎平滑，梯度很小，不利于权重更新**；不同的是输出区间，tanh的输出区间是在**(-1,1)**之间，而且整个函数是以0为中心的，这个特点比sigmod的好。

  ![img](file:////tmp/wps-yeapht/ksohtml/wpskyYudl.jpg)    ![img](file:////tmp/wps-yeapht/ksohtml/wpsTt4CkX.jpg)

### 深度学习过拟合问题

- **参数添加约束**

  -  **L1范数**：参数约束添加L1范数惩罚项  ![img](file:////tmp/wps-yeapht/ksohtml/wps6TuMrz.jpg)

  -  **L2范数**： 参数约束添加L2范数惩罚项 ![img](file:////tmp/wps-yeapht/ksohtml/wpspSSWyb.jpg)

-  **Dropout**

  一类通用的正则化方法， Dropout在训练过程中**随机的丢弃一部分输入**，此时丢弃部分对应的参数不会更新

  1. **相当于Dropout是一个集成方法：**
  - 将所有**子网络**结果进行合并，通过随机丢弃
  - 输入可以得到各种子网络

  2. **通过不同的输入屏蔽相当于学习到所有子网络结构。**
  -  因此前向传播过程变成如下形式，相当于每层输入多了一个屏蔽向量μ
  - 来控制该层有哪些输入会被屏蔽掉。

- **扩充训练集**

​    **防止过拟合最有效的方法是增加训练集合，训练集合越大过拟合概率越小**。数据集合扩充是一个省时有效的方法，但是在不同领域方法不太通用

    1. **数据变换**：在目标识别领域常用的方法是将图片进行**旋转**、**缩放**、**裁剪**等
    2. **添加噪声：**可以对输入添加噪声，也可以对隐藏层或者输出层添加噪声(Noise)
    3. **词义替换：**NLP中常用思路是进行近义词替换

 



## 神经网络

### **人工神经网络概述**

### 人工神经网络发展历程

### 深度信念网络

- BM（玻尔兹曼机）

  

- RBM（受限玻尔兹曼机）

  

- DBN（深度信念网络）


### 堆积的自动编码器

### CNN（卷积神经网络）

  演进

### 卷积自动编码器

### 神经网络的计算

- **初始化网络结构：**

![img](file:////tmp/wps-yeapht/ksohtml/wpsT0HaGN.jpg) 

- **前向传播：**

(h = x1*w1 + ... + xn * wn + b   E=(1/n)*（target-output）^2)，激活函数为sigmod=1/(1+e^-(wx+b))，n为样本数，此处为2

h1 = 0.05*0.15+0.10*0.20+0.35*1=0.3775       o1=0.3775*0.40+0.3925*0.45+0.60*1=0.927625  e1=0.5*(0.01-0.927625)

h2 = 0.05*0.25+0.10*0.30+0.35*1=0.3925       o2=0.3775*0.50+0.3925*0.55+0.60*1=1.004625 

-  **反向传播：（链式求导）**

 

## 卷积神经网络的演进

|  CNN网络  |   提出者   | 时间 |      网络层数       |              网络特点               |
| :-------: | :--------: | :--: | :-----------------: | :---------------------------------: |
|   LeNet   | Yang Lecun | 1999 | 2 conv + 2 pool = 4 |          过拟合；计算量大           |
|  AlexNet  |   Hiton    | 2012 | 5 conv + 3 pool = 8 | 数据增强；Relu；Dropout；LRN；多GPU |
|    VGG    |    剑桥    | 2014 |        16/19        |          卷积组；卷积变小           |
| GoogleNet | Google团队 | 2014 |         22          |   卷积分支；1*1卷积；BN；卷积分解   |
|  ResNet   |   何凯明   | 2015 |         152         |           参差；瓶颈结构            |

> 精度越来越高，层次越来越深，模型越来越复杂，参数越来越多

### LeNet网络





### AlexNet网络





### VGG网络





### GoogleNet网络





### ResNet网络

 

## 循环神经网络

### BiRNN网络

### LSTM网络

### Seq2seq网络

 

## 深度学习框架

 

## 深度学习优化算法

### 优化与深度学习

* **目标函数**

* **损失函数**

* **目的**

  **最小化目标函数**：由于优化算法的⽬标函数通常是⼀个基于训练数据集的损失函数，优化的⽬标在于降低训练误差。而**深度学习的⽬标在于降低泛化误差**。为了降低泛化误差，除了使⽤优化算法降低训练误差以外，我们还需要注意应对**过拟合**。

* **挑战**

**局部最小值、鞍点**

### 梯度下降和

* **梯度下降**

-  **随机梯度下降**


* **小批量随机梯度下降**

### 动量法

 

### Adagrad

 

### RMSProp

 

### Adadelta


### Adam